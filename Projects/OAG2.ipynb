{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================切换目录=====================================\n",
      "/home/shichaoyu/perl5/IRpro2\n",
      "/home/shichaoyu/perl5/IRpro2/dataset\n",
      "['train', 'cat_feature_index.3c4c9673-11b0f6ac-cb85fb96-91833486.tmp', 'catboost_info', 'cat_feature_index.40b30d8b-f30e5088-cc5f7d50-21dddf23.tmp', 'out', 'cna_test_data', 'cna_data', 'pkl2', 'cat_feature_index.45ed2826-847e7041-c540e66c-339d9b99.tmp', 'cat_feature_index.3be0c692-8a8fe0c8-bb8954c3-56fb0ff.tmp', 'feat']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"=================================切换目录=====================================\")\n",
    "print(os.getcwd()) # 打印当前工作目录\n",
    "os.chdir('/home/shichaoyu/perl5/IRpro2/dataset/')\n",
    "print(os.getcwd())\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. to_pickle: 将数据存为表格格式\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "pkl_path = './pkl2'\n",
    "if not os.path.exists(pkl_path):\n",
    "    os.mkdir(pkl_path)\n",
    "\n",
    "    \n",
    "# 测试集实例提交文件    cna_test_data\n",
    "with open(\"cna_test_data/test_example_evaluation_continuous.json\")as file:\n",
    "    valid_example=json.load(file, object_pairs_hook=OrderedDict)\n",
    "# 待分类文件\n",
    "with open(\"cna_test_data/cna_test_unass_competition.json\")as file:\n",
    "    cna_valid_unass=json.load(file, object_pairs_hook=OrderedDict)\n",
    "#  分类文件元数据\n",
    "with open(\"cna_test_data/cna_test_pub.json\")as file:\n",
    "    cna_valid_pub=json.load(file, object_pairs_hook=OrderedDict)\n",
    "    \n",
    "# #示例提交文件\n",
    "# with open('cna_data/valid_example_evaluation_continuous.json') as file:\n",
    "#     valid_example = json.load(file, object_pairs_hook=OrderedDict)\n",
    "# # 论文元信息\n",
    "# with open('cna_data/cna_valid_pub.json') as file:\n",
    "#     cna_valid_pub = json.load(file, object_pairs_hook=OrderedDict)\n",
    "    \n",
    "# #  待分配的论文列表\n",
    "# with open('cna_data/cna_valid_unass_competition.json') as file:\n",
    "#     cna_valid_unass = json.load(file, object_pairs_hook=OrderedDict)\n",
    "\n",
    "# 作者档案    已有用户档案\n",
    "with open('cna_data/whole_author_profile.json') as file:\n",
    "    whole_author_profile = json.load(file, object_pairs_hook=OrderedDict)\n",
    "#     二级字典，key值为作者id，value分为俩个域: ‘name’域代表作者名，’papers’域代表作者的所拥有的论文(作者的profile)\n",
    "#    , 测试集与验证集使用同一个已有的作者档案；\n",
    "\n",
    "# 涉及的论文元信息\n",
    "with open('cna_data/whole_author_profile_pub.json') as file:\n",
    "    whole_author_profile_pub = json.load(file, object_pairs_hook=OrderedDict)\n",
    "\n",
    "# 训练集   二级字典   作者姓名，作者ID，论文ID列表   一个作者对应许多ID，一个ID对应多篇论文\n",
    "# 论文元数据\n",
    "with open('train/train_pub.json') as file:\n",
    "    train_pub = json.load(file, object_pairs_hook=OrderedDict)\n",
    "with open('train/train_author.json') as file:\n",
    "    train_author = json.load(file, object_pairs_hook=OrderedDict)\n",
    "\n",
    "#  将数据成对保存，使用pandas处理，并使用pickle保存\n",
    "### train_author\n",
    "\n",
    "# 1. author_name, author_ids    作者姓名和ID  \n",
    "train_author_names = list(train_author.keys())\n",
    "train_author_ids = [list(v.keys()) for v in train_author.values()]\n",
    "train_author_name_ids = pd.DataFrame(list(zip(train_author_names, train_author_ids)), columns=['author_name', 'author_ids'])\n",
    "\n",
    "# train_author_name_ids.head()\n",
    "\n",
    "train_author_name_ids.to_pickle('./pkl2/train_author_name_ids.pkl')\n",
    "\n",
    "# 2. author_id, paper_ids   作者ID和论文ID  \n",
    "train_author_paper_ids = pd.DataFrame([(k2, v2) for v1 in train_author.values() for k2, v2 in v1.items()], columns=['author_id', 'paper_ids'])\n",
    "\n",
    "# train_author_paper_ids.head()\n",
    "\n",
    "train_author_paper_ids.to_pickle('./pkl2/train_author_paper_ids.pkl')\n",
    "\n",
    "### train_pub\n",
    "\n",
    "# paper_id, author_names&orgs, title, venue, year, keywords, abstract\n",
    "train_pub_info = pd.DataFrame.from_dict(train_pub, orient='index').reset_index(drop=True).rename({'id':'paper_id'}, axis=1)\n",
    "\n",
    "# train_pub_info.head()\n",
    "\n",
    "train_pub_info.to_pickle('./pkl2/train_pub_info.pkl')\n",
    "\n",
    "### whole_author_profile\n",
    "\n",
    "# 其键（key）是论文ID，其值是相应的论文信息。 \n",
    "\n",
    "whole_author_name_paper_ids = pd.DataFrame.from_dict(whole_author_profile, orient='index').reset_index()\n",
    "whole_author_name_paper_ids.columns = ['author_id', 'author_name', 'paper_ids']\n",
    "\n",
    "# whole_author_name_paper_ids.head()\n",
    "\n",
    "whole_author_name_paper_ids.to_pickle('./pkl2/whole_author_name_paper_ids.pkl')\n",
    "\n",
    "### whole_author_profile_pub\n",
    "\n",
    "# paper_id, author_names&orgs, title, venue, year, keywords, abstract\n",
    "whole_pub_info = pd.DataFrame.from_dict(whole_author_profile_pub, orient='index').reset_index(drop=True).rename({'id':'paper_id'}, axis=1)\n",
    "\n",
    "# whole_pub_info.head()\n",
    "\n",
    "whole_pub_info.to_pickle('./pkl2/whole_pub_info.pkl')\n",
    "\n",
    "### cna_valid_unass\n",
    "\n",
    "# 论文列表，代表待分配的论文list，列表中的元素为论文id + ‘-’ + 需要分配的作者index(从0开始)；\n",
    "# 参赛者需要将该文件中的每篇论文的待分配作者对应分配到已有作者档案中(whole_author_profile.json).\n",
    "\n",
    "cna_valid_unass = pd.DataFrame(cna_valid_unass, columns=['cna_valid_unass'])\n",
    "\n",
    "cna_valid_unass['cna_valid_unass'] = cna_valid_unass['cna_valid_unass'].apply(lambda x: x.split('-'))\n",
    "\n",
    "cna_valid_unass['paper_id'] = cna_valid_unass['cna_valid_unass'].apply(lambda x: x[0])\n",
    "cna_valid_unass['author_idx'] = cna_valid_unass['cna_valid_unass'].apply(lambda x: x[1])\n",
    "\n",
    "del cna_valid_unass['cna_valid_unass']\n",
    "\n",
    "# cna_valid_unass.head()\n",
    "\n",
    "cna_valid_unass.to_pickle('./pkl2/cna_valid_unass.pkl')\n",
    "\n",
    "### cna_valid_pub\n",
    "\n",
    "# paper_id, author_names&orgs, title, venue, year, keywords, abstract\n",
    "valid_pub_info = pd.DataFrame.from_dict(cna_valid_pub, orient='index').reset_index(drop=True).rename({'id':'paper_id'}, axis=1)\n",
    "\n",
    "# valid_pub_info.head()\n",
    "\n",
    "valid_pub_info.to_pickle('./pkl2/valid_pub_info.pkl')\n",
    "\n",
    "### pub info\n",
    "\n",
    "pub_info = pd.concat([whole_pub_info, train_pub_info, valid_pub_info]).drop_duplicates(subset='paper_id', keep='first')\n",
    "\n",
    "pub_info['orgs'] = pub_info['authors'].apply(lambda x: [ao['org'] for ao in x if 'org' in ao])\n",
    "pub_info['authors'] = pub_info['authors'].apply(lambda x: [ao['name'] for ao in x if 'name' in ao])\n",
    "\n",
    "pub_info['year'] = pub_info['year'].fillna(0).replace('', 0).astype(int)\n",
    "\n",
    "pub_info['abstract'] = pub_info['abstract'].fillna(' ').replace('', ' ')\n",
    "\n",
    "# pub_info.head()\n",
    "\n",
    "pub_info.to_pickle('./pkl2/pub_info.pkl')\n",
    "\n",
    "### author_pub_detail\n",
    "\n",
    "author_pub_ids = whole_author_name_paper_ids[['author_id','paper_ids']].merge(train_author_paper_ids, 'left', 'author_id')\n",
    "\n",
    "author_pub_ids['paper_ids_x_len'] = author_pub_ids['paper_ids_x'].apply(len)\n",
    "author_pub_ids['paper_ids_y_len'] = author_pub_ids['paper_ids_y'].apply(lambda x: 0 if type(x) == float else len(x))\n",
    "\n",
    "author_pub_ids['paper_ids'] = author_pub_ids.apply(lambda row: list(set(row['paper_ids_x']) | (set() if type(row['paper_ids_y']) == float else set(row['paper_ids_y']))), axis=1)\n",
    "\n",
    "author_pub_ids['paper_ids_len'] = author_pub_ids['paper_ids'].apply(len)\n",
    "\n",
    "author_pub_ids.drop(columns=['paper_ids_x', 'paper_ids_y', 'paper_ids_x_len', 'paper_ids_y_len'], inplace=True)\n",
    "\n",
    "# author_pub_ids.head()\n",
    "\n",
    "author_pub_ids['paper_ids_len'].describe()\n",
    "\n",
    "pub_info = pub_info.set_index('paper_id')\n",
    "\n",
    "# pub_info.head()\n",
    "\n",
    "# author_id paper_ids paper_ids_len abstracts keywords titles venues years authors orgs\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "author_pub_ids_ = author_pub_ids[['author_id', 'paper_ids']].values\n",
    "pub_col = ['abstract', 'keywords', 'title', 'venue', 'year', 'authors', 'orgs']\n",
    "for pc in pub_col:\n",
    "    print(pc)\n",
    "    dat = []\n",
    "    for author_id, paper_ids in tqdm_notebook(author_pub_ids_):\n",
    "        d = []\n",
    "        for pid in paper_ids:\n",
    "            d.append(pub_info.loc[pid, pc])\n",
    "        dat.append(d)\n",
    "    author_pub_ids[pc] = dat\n",
    "\n",
    "# author_pub_ids.head()\n",
    "\n",
    "len(author_pub_ids[author_pub_ids['author_id'] == 'sCKCrny5']['abstract'].values[0])\n",
    "\n",
    "author_pub_ids['year'].apply(len).describe()\n",
    "\n",
    "author_pub_ids.to_pickle('./pkl2/author_pub_detail.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2841\n"
     ]
    }
   ],
   "source": [
    "## 2. gen_valid 生成测试样本\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "\n",
    "### valid data\n",
    "\n",
    "valid_pub_info = pd.read_pickle('./pkl2/valid_pub_info.pkl')      #pub信息\n",
    "cna_valid_unass = pd.read_pickle('./pkl2/cna_valid_unass.pkl')    # 验证集信息\n",
    "\n",
    "valid_data = cna_valid_unass.merge(valid_pub_info, 'left', 'paper_id')\n",
    "\n",
    "valid_data['author_idx'] = valid_data['author_idx'].astype(int)\n",
    "print(valid_data['author_idx'].max())\n",
    "\n",
    "valid_data['author_name'] = valid_data.apply(lambda row: row['authors'][row['author_idx']]['name'], axis=1)\n",
    "\n",
    "valid_data['author_org'] = valid_data.apply(lambda row: row['authors'][row['author_idx']].get('org'), axis=1)\n",
    "\n",
    "valid_data = valid_data[['paper_id', 'author_name', 'author_org']]\n",
    "# 将作者姓名转换为统一格式 ，名中间不保留任何字符，姓氏和名字之间保留一个下划线字符\n",
    "def convert(name):\n",
    "    name = name.lower()\n",
    "    name = name.replace('. ', '_').replace('.', '_').replace(' ', '_').replace('-', '')\n",
    "    if name in ['yang_jie', 'jie_yang_0002', 'jie\\xa0yang', 'jie_yang_0008']:\n",
    "        name = 'jie_yang'\n",
    "    if name in ['liu_bing']:\n",
    "        name = 'bing_liu'\n",
    "    return name\n",
    "\n",
    "valid_data['author_name'] = valid_data['author_name'].apply(convert)\n",
    "\n",
    "# valid_data.head()\n",
    "\n",
    "valid_data['author_name'].nunique()\n",
    "\n",
    "valid_data[valid_data['author_org'] == 'South China University of Technology']\n",
    "\n",
    "\n",
    "\n",
    "whole_author_name_paper_ids = pd.read_pickle('./pkl2/whole_author_name_paper_ids.pkl')\n",
    "\n",
    "whole_author_name_paper_ids.head()\n",
    "\n",
    "valid_data_ = valid_data.merge(whole_author_name_paper_ids[['author_name', 'author_id']], 'left', 'author_name')\n",
    "\n",
    "valid_data_.isnull().sum() / len(valid_data_)\n",
    "\n",
    "# valid_data_.head()\n",
    "\n",
    "valid_data_.to_pickle('./pkl2/valid_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(475195, 7)\n",
      "(271719, 7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a96e3f326814306a0565f4b06a9fcdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=271719), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "王慧\n",
      "刘晶 liu jing\n",
      "刘建国 liu jianguo\n",
      "胡斌\n",
      "----------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2164"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 3. author_name_match 作者名匹配\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pypinyin\n",
    "\n",
    "# !pip install pypinyin\n",
    "# 检测中文，Unicode编码 \n",
    "def check_chs(c):\n",
    "    return '\\u4e00' <= c <= '\\u9fa5'\n",
    "\n",
    "def to_pinyin(word):\n",
    "    s = ''\n",
    "    for i in pypinyin.pinyin(word, style=pypinyin.NORMAL):\n",
    "        s += ''.join(i)\n",
    "    return s\n",
    "\n",
    "##   全部信息\n",
    "whole_pub_info = pd.read_pickle('./pkl2/whole_pub_info.pkl')\n",
    "whole_author_name_paper_ids = pd.read_pickle('./pkl2/whole_author_name_paper_ids.pkl')\n",
    "##   训练集信息\n",
    "train_pub_info = pd.read_pickle('./pkl2/train_pub_info.pkl')\n",
    "train_author_name_ids = pd.read_pickle('./pkl2/train_author_name_ids.pkl')\n",
    "train_author_paper_ids = pd.read_pickle('./pkl2/train_author_paper_ids.pkl')\n",
    "##   验证集信息\n",
    "valid_pub_info = pd.read_pickle('./pkl2/valid_pub_info.pkl')\n",
    "cna_valid_unass = pd.read_pickle('./pkl2/cna_valid_unass.pkl')\n",
    "\n",
    "# 将数据索引对齐，进行合并\n",
    "pub_info = pd.concat([whole_pub_info, train_pub_info, valid_pub_info])\n",
    "print(pub_info.shape)\n",
    "# 去除重复项\n",
    "pub_info = pub_info.drop_duplicates(subset='paper_id', keep='first')\n",
    "print(pub_info.shape)\n",
    "\n",
    "whole_author_name_paper_ids.head()\n",
    "\n",
    "paper_authors = {}\n",
    "for author_name, paper_ids in whole_author_name_paper_ids[['author_name', 'paper_ids']].values:\n",
    "    for pid in paper_ids:\n",
    "        if not pid in paper_authors:\n",
    "            paper_authors[pid] = [author_name]\n",
    "        else:\n",
    "            paper_authors[pid].append(author_name)\n",
    "\n",
    "paper_authors_df = pd.DataFrame([(k, v) for k,v in paper_authors.items()], columns=['paper_id', 'author_ids'])\n",
    "\n",
    "pub_info['author_names'] = pub_info['authors'].apply(lambda x: [ao['name'] for ao in x])\n",
    "\n",
    "pub_info = pub_info.merge(paper_authors_df, 'left', 'paper_id')\n",
    "\n",
    "pub_info.head()\n",
    "\n",
    "from collections import defaultdict\n",
    "def score(n1, n2):\n",
    "    n1 = ''.join(filter(str.isalpha, n1.lower()))\n",
    "    if check_chs(n1):\n",
    "#         print(n1)\n",
    "        n1 = to_pinyin(n1)\n",
    "#         print(n1)\n",
    "    n2 = ''.join(filter(str.isalpha, n2.lower()))\n",
    "    counter = defaultdict(int)\n",
    "    score = 0\n",
    "    for c in n1:\n",
    "        counter[c] += 1\n",
    "    for c in n2:\n",
    "        if (c in counter) and (counter[c] > 0):\n",
    "            counter[c] -= 1\n",
    "        else:\n",
    "            score += 1\n",
    "    score += np.sum(list(counter.values()))\n",
    "    return score\n",
    "    \n",
    "score('hello world', 'world hello')\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "author_name_map = {}\n",
    "for author_names, author_ids in tqdm_notebook(pub_info[['author_names', 'author_ids']].values):\n",
    "    if type(author_ids) == float:\n",
    "        continue\n",
    "    for aid in author_ids:\n",
    "        dis = []\n",
    "        for an in author_names:\n",
    "            dis.append(score(an, aid))\n",
    "        cor = author_names[np.argmin(dis)]\n",
    "        author_name_map[cor] = aid\n",
    "\n",
    "for k in author_name_map.keys():\n",
    "    if check_chs(k):\n",
    "        print(k)\n",
    "\n",
    "import pickle\n",
    "with open('./pkl2/author_name_map.pkl', 'wb') as file:\n",
    "    pickle.dump(author_name_map, file)\n",
    "print(82*'-')\n",
    "author_name_map\n",
    "\n",
    "len(author_name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(475195, 7)\n",
      "(271719, 7)\n",
      "  author_id                                          paper_ids\n",
      "0  EShnTfSe                                         [P9a1gcvg]\n",
      "1  sCKCrny5  [Rg5fAeTd, lJPsGNBE, er5gTz90, UG32p2zs, FTweB...\n",
      "   paper_id author_name                                         author_org  \\\n",
      "0  5Ix5q3pI   xiong_hui                                               None   \n",
      "1  8X1mND8h  zhang_tong  The College of Life Science,Henan University,K...   \n",
      "\n",
      "  author_id  \n",
      "0       NaN  \n",
      "1       NaN  \n",
      "  author_id                                          paper_ids author_name\n",
      "0  EShnTfSe                                         [P9a1gcvg]      li_guo\n",
      "1  sCKCrny5  [Rg5fAeTd, lJPsGNBE, er5gTz90, UG32p2zs, FTweB...      li_guo\n",
      "  author_id author_name                                          paper_ids\n",
      "0  EShnTfSe      li_guo                                         [P9a1gcvg]\n",
      "1  sCKCrny5      li_guo  [Rg5fAeTd, lJPsGNBE, er5gTz90, UG32p2zs, FTweB...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6405ae79a2dc4e53a89bf2de1767565c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=48750), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## 4. author_org_match 作者单位匹配\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "\n",
    "##    全部信息\n",
    "whole_pub_info = pd.read_pickle('./pkl2/whole_pub_info.pkl')\n",
    "whole_author_name_paper_ids = pd.read_pickle('./pkl2/whole_author_name_paper_ids.pkl')\n",
    "##   训练集\n",
    "train_pub_info = pd.read_pickle('./pkl2/train_pub_info.pkl')\n",
    "train_author_name_ids = pd.read_pickle('./pkl2/train_author_name_ids.pkl')\n",
    "train_author_paper_ids = pd.read_pickle('./pkl2/train_author_paper_ids.pkl')\n",
    "##   验证集\n",
    "valid_pub_info = pd.read_pickle('./pkl2/valid_pub_info.pkl')\n",
    "valid_data = pd.read_pickle('./pkl2/valid_data.pkl')\n",
    "#   合并\n",
    "pub_info = pd.concat([whole_pub_info, train_pub_info, valid_pub_info])\n",
    "print(pub_info.shape)\n",
    "pub_info = pub_info.drop_duplicates(subset='paper_id', keep='first')\n",
    "print(pub_info.shape)\n",
    "\n",
    "pub_info.head(2)\n",
    "\n",
    "pub_info = pub_info.set_index('paper_id')\n",
    "pub_info.head(2)\n",
    "\n",
    "pub_info.loc['0009qJgC', 'authors']\n",
    "\n",
    "with open('./pkl2/author_name_map.pkl', 'rb') as file:\n",
    "    author_name_map = pickle.load(file)\n",
    "\n",
    "whole_author_name_paper_ids.head(2)\n",
    "\n",
    "print(train_author_paper_ids.head(2))\n",
    "\n",
    "print(valid_data.head(2))\n",
    "\n",
    "# dict: {author_name:{paper_id:org}}\n",
    "\n",
    "train_author_name_ids = pd.read_pickle('./pkl2/train_author_name_ids.pkl')\n",
    "\n",
    "train_author_name_ids_ext = []\n",
    "for author_name, author_ids in train_author_name_ids[['author_name', 'author_ids']].values:\n",
    "     for aid in author_ids:\n",
    "            train_author_name_ids_ext.append([author_name, aid])\n",
    "            \n",
    "train_author_name_ids_ext = pd.DataFrame(train_author_name_ids_ext, columns=['author_name', 'author_id'])\n",
    "train_author_name_ids_ext.head()\n",
    "\n",
    "train_author_name_paper_ids = train_author_paper_ids.merge(train_author_name_ids_ext, 'left', 'author_id')\n",
    "print(train_author_name_paper_ids.head(2))\n",
    "\n",
    "author_name_paper_ids = pd.concat([train_author_name_paper_ids, whole_author_name_paper_ids])\n",
    "print(author_name_paper_ids.head(2))\n",
    "\n",
    "# dict: {author_name:{paper_id:org}}\n",
    "from tqdm import tqdm_notebook\n",
    "author_org_map = {}\n",
    "for author_name, paper_ids in tqdm_notebook(author_name_paper_ids[['author_name', 'paper_ids']].values):\n",
    "    if not author_name in author_org_map:\n",
    "        author_org_map[author_name] = {}\n",
    "    for pid in paper_ids:\n",
    "        org = np.nan\n",
    "        author_orgs = pub_info.loc[pid, 'authors']\n",
    "        for ao in author_orgs:\n",
    "            if not ao['name'] in author_name_map:\n",
    "                continue\n",
    "            if author_name_map[ao['name']] == author_name:\n",
    "                org = ao.get('org')\n",
    "        author_org_map[author_name][pid] = org\n",
    "\n",
    "author_org_map['li_guo']\n",
    "\n",
    "author_org_map['li_guo']['UG32p2zs']\n",
    "\n",
    "with open('./pkl2/author_org_map.pkl', 'wb') as file:\n",
    "    pickle.dump(author_org_map, file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  186\n",
      "train:  221\n",
      "whole:  320\n",
      "------------------------------------------------------------\n",
      "valid & train:  1\n",
      "whole & valid:  65\n",
      "whole & train:  220\n",
      "2\n",
      "CPU times: user 17 s, sys: 1.33 s, total: 18.3 s\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## 5 gen_train 生成训练样本\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "import pickle\n",
    "with open('./pkl2/author_name_map.pkl', 'rb') as file:\n",
    "    author_name_map = pickle.load(file)\n",
    "with open('./pkl2/author_org_map.pkl', 'rb') as file:\n",
    "    author_org_map = pickle.load(file)\n",
    "\n",
    "whole_author_name_paper_ids = pd.read_pickle('./pkl2/whole_author_name_paper_ids.pkl')\n",
    "train_author_name_ids = pd.read_pickle('./pkl2/train_author_name_ids.pkl')\n",
    "valid_data = pd.read_pickle('./pkl2/valid_data.pkl')\n",
    "\n",
    "print('valid: ', len(set(valid_data['author_name'])))\n",
    "print('train: ', len(set(train_author_name_ids['author_name'])))\n",
    "print('whole: ', len(set(whole_author_name_paper_ids['author_name'])))\n",
    "print('-'*60)\n",
    "print('valid & train: ', len(set(train_author_name_ids['author_name']) & set(valid_data['author_name'])))\n",
    "print('whole & valid: ', len(set(whole_author_name_paper_ids['author_name']) & set(valid_data['author_name'])))\n",
    "print('whole & train: ', len(set(whole_author_name_paper_ids['author_name']) & set(train_author_name_ids['author_name'])))\n",
    "\n",
    "train_author_paper_ids = pd.read_pickle('./pkl2/train_author_paper_ids.pkl')\n",
    "\n",
    "# whole_author_name_paper_ids\n",
    "\n",
    "# train_author_name_ids\n",
    "\n",
    "# train_author_paper_ids\n",
    "\n",
    "train_author_name_ids['author_num'] = train_author_name_ids['author_ids'].apply(len)\n",
    "#  数量大于2 \n",
    "train_author_name_ids = train_author_name_ids[train_author_name_ids['author_num'] >= 2]\n",
    "\n",
    "print(train_author_name_ids['author_num'].min())\n",
    "\n",
    "train_author_name_ids\n",
    "\n",
    "train_author_name_ids_ext = []\n",
    "for author_name, author_ids in train_author_name_ids[['author_name', 'author_ids']].values:\n",
    "     for aid in author_ids:\n",
    "            train_author_name_ids_ext.append([author_name, aid])\n",
    "            \n",
    "train_author_name_ids_ext = pd.DataFrame(train_author_name_ids_ext, columns=['author_name', 'author_id'])\n",
    "train_author_name_ids_ext.head()\n",
    "\n",
    "train_author_paper_ids_ext = []\n",
    "for author_id, paper_ids in train_author_paper_ids[['author_id', 'paper_ids']].values:\n",
    "     for pid in paper_ids:\n",
    "            train_author_paper_ids_ext.append([author_id, pid])\n",
    "train_author_paper_ids_ext = pd.DataFrame(train_author_paper_ids_ext, columns=['author_id', 'paper_id'])\n",
    "\n",
    "# train_pub_info = pd.read_pickle('./pkl/train_pub_info.pkl')[['paper_id', 'authors']]\n",
    "\n",
    "# train_author_paper_ids_ext = train_author_paper_ids_ext.merge(train_pub_info, 'left', 'paper_id')\n",
    "\n",
    "train_author_paper_ids_ext = train_author_paper_ids_ext.merge(train_author_name_ids_ext, 'left', 'author_id')\n",
    "\n",
    "train_author_paper_ids_ext.head()\n",
    "\n",
    "\n",
    "train_author_paper_ids_ext['author_org'] = train_author_paper_ids_ext.apply(lambda row: author_org_map[row['author_name']][row['paper_id']], axis=1)\n",
    "\n",
    "train_author_paper_ids_ext['author_org'].nunique() / len(train_author_paper_ids_ext)\n",
    "\n",
    "train_author_paper_ids_ext.head()\n",
    "\n",
    "train_author_name_ids_ext.head()\n",
    "\n",
    "train_author_name_ids_ext2 = train_author_name_ids_ext.merge(train_author_name_ids[['author_name', 'author_ids']], 'left', 'author_name')\n",
    "\n",
    "train_author_name_ids_ext2.head()\n",
    "\n",
    "# sample 采样的方式！！！\n",
    "n = 15\n",
    "train_author_name_ids_ext2['author_ids_sample'] = train_author_name_ids_ext2['author_ids'].apply(lambda x: np.random.permutation(x)[:n])\n",
    "\n",
    "train_author_name_ids_ext2['author_ids_sample'] = train_author_name_ids_ext2.apply(lambda row: {row['author_id']} | set(row['author_ids_sample']), axis=1)\n",
    "train_author_name_ids_ext2.head()\n",
    "\n",
    "train_author_ids_ext2_sample = []\n",
    "for author_id, author_ids_sample in train_author_name_ids_ext2[['author_id', 'author_ids_sample']].values:\n",
    "     for aid in author_ids_sample:\n",
    "            train_author_ids_ext2_sample.append([author_id, aid])\n",
    "            \n",
    "train_author_ids_ext2_sample = pd.DataFrame(train_author_ids_ext2_sample, columns=['author_id', 'author_id_sample'])\n",
    "train_author_ids_ext2_sample.head()\n",
    "\n",
    "train_data = train_author_paper_ids_ext.merge(train_author_ids_ext2_sample, 'left', 'author_id')\n",
    "\n",
    "train_data['label'] = (train_data['author_id'] == train_data['author_id_sample']).astype(int)\n",
    "\n",
    "train_data.drop(columns=['author_id'], inplace=True)\n",
    "\n",
    "train_data.columns = ['paper_id', 'author_name', 'author_org', 'author_id', 'label']\n",
    "\n",
    "train_data['label'].value_counts()\n",
    "\n",
    "train_data.head()\n",
    "\n",
    "train_data.to_pickle('./pkl2/train_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3142238, 5)\n",
      "(505765, 4)\n",
      "(3648003, 21)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c0d29351c34e169b3a16edff42dab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3648003), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Index(['author_id', 'author_name', 'author_org', 'label', 'paper_id',\n",
      "       'abstract_a', 'authors_a', 'keywords_a', 'title_a', 'venue_a', 'year_a',\n",
      "       'orgs_a', 'paper_ids', 'paper_ids_len', 'abstract_b', 'keywords_b',\n",
      "       'title_b', 'venue_b', 'year_b', 'authors_b', 'orgs_b', 'idx'],\n",
      "      dtype='object')\n",
      "==================================================================================\n",
      "----------------------------------------------------------------------------------\n",
      "CPU times: user 16min 32s, sys: 1min 16s, total: 17min 48s\n",
      "Wall time: 16min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## 6 gen_feat_v1 构造特征\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "\n",
    "feat_dir = './feat/'\n",
    "if not os.path.exists(feat_dir):\n",
    "    os.mkdir(feat_dir)\n",
    "\n",
    "train_data = pd.read_pickle('./pkl2/train_data.pkl')\n",
    "test_data = pd.read_pickle('./pkl2/valid_data.pkl')\n",
    "\n",
    "pub_info = pd.read_pickle('./pkl2/pub_info.pkl')\n",
    "author_pub_detail = pd.read_pickle('./pkl2/author_pub_detail.pkl')\n",
    "\n",
    "train_data.head(3)\n",
    "\n",
    "test_data.head(3)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "data = pd.concat([train_data, test_data]).reset_index(drop=True)\n",
    "\n",
    "data.head()\n",
    "\n",
    "pub_info.columns = ['abstract_a', 'authors_a', 'keywords_a', 'paper_id', 'title_a', 'venue_a', 'year_a', 'orgs_a']\n",
    "pub_info.head()\n",
    "\n",
    "author_pub_detail.columns = ['author_id', 'paper_ids', 'paper_ids_len', 'abstract_b', 'keywords_b', 'title_b', 'venue_b', 'year_b', 'authors_b', 'orgs_b']\n",
    "author_pub_detail.head()\n",
    "\n",
    "data = data.merge(pub_info, 'left', 'paper_id').merge(author_pub_detail, 'left', 'author_id')\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "data.head(3)\n",
    "\n",
    "### delete paper_id in pos sample\n",
    "\n",
    "def pidx(p, ps):\n",
    "    ans = np.nan\n",
    "    for i, p2 in enumerate(ps):\n",
    "        if p == p2:\n",
    "            ans = i\n",
    "            break\n",
    "    return ans\n",
    "\n",
    "\n",
    "data['idx'] = data.apply(lambda row: pidx(row['paper_id'], row['paper_ids']) if row['label'] == 1 else np.nan, axis=1)\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "cols = ['abstract_b', 'keywords_b', 'title_b', 'venue_b', 'year_b', 'authors_b', 'orgs_b']\n",
    "for i in tqdm_notebook(range(len(data))):\n",
    "    if pd.isna(data.loc[i, 'idx']):\n",
    "        continue\n",
    "    for c in cols:\n",
    "        v = list(data.loc[i, c])\n",
    "        del v[data.loc[i, 'idx'].astype(int)]\n",
    "        data.set_value(i, c, v)\n",
    "#         data.loc[i, c] = frozenset(v)\n",
    "\n",
    "data.to_pickle('./pkl2/data.pkl')\n",
    "\n",
    "print(data.columns)\n",
    "print(82*'=')\n",
    "\n",
    "\n",
    "### year\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "# print(np.nan)\n",
    "data = pd.read_pickle('./pkl2/data.pkl')  # 1388713\n",
    "# print(data.head(1))\n",
    "# print(data['year_b'].shape[0])\n",
    "# print(len(data['year_b'][1]))\n",
    "def myfunc(x):\n",
    "    if isinstance(x,float):\n",
    "        x=[0]\n",
    "        return x\n",
    "    else:\n",
    "        if len(x)==0:\n",
    "            x=[0]\n",
    "        return x\n",
    "# for i in range(2185631,2185640):\n",
    "# #     if math.isnan(data['year_b'][i]):\n",
    "# #         print(i)\n",
    "#     if isinstance(data['year_b'][i],float):\n",
    "#         print(i)\n",
    "#     print(type(data['year_b'][i]))\n",
    "#     print(data['year_b'][i])\n",
    "# print(data['year_b'][2185634])\n",
    "# for i in range(1388713,data['year_b'].shape[0]):\n",
    "#     print(i)\n",
    "#     if len(data['year_b'][i])==0:\n",
    "#         data['year_b'][i]=[0]\n",
    "# print(data['year_b'].head(2))\n",
    "data['year_b'] = data['year_b'].apply(lambda x: myfunc(x))\n",
    "\n",
    "data['year_b_min'] = data['year_b'].apply(np.min)\n",
    "data['year_b_max'] = data['year_b'].apply(np.max)\n",
    "data['year_b_mean'] = data['year_b'].apply(np.mean)\n",
    "data['year_b_std'] = data['year_b'].apply(np.std)\n",
    "\n",
    "data['year_b_mm2'] = (data['year_b_min'] + data['year_b_max']) / 2\n",
    "data['year_b_ai2'] = (data['year_b_max'] - data['year_b_min']) \n",
    "\n",
    "for c in ['year_b_min', 'year_b_max', 'year_b_mean', 'year_b_mm2']:\n",
    "    data[c + '-year_a'] = data[c] - data['year_a']\n",
    "\n",
    "data['year_inside_range'] = ((data['year_b_min-year_a'] <= 0) & (data['year_b_max-year_a'] >= 0)).astype(int)\n",
    "\n",
    "cols = ['year_a', 'year_b_min', 'year_b_max', 'year_b_mean', 'year_b_std', 'year_b_mm2',\n",
    "       'year_b_min-year_a', 'year_b_max-year_a', 'year_b_mean-year_a',\n",
    "       'year_b_mm2-year_a', 'year_inside_range','year_b_ai2']\n",
    "\n",
    "data[cols].to_pickle('./feat/time_feat_a.pkl')\n",
    "\n",
    "\n",
    "### 'authors', 'orgs'\n",
    "\n",
    "tmp = data[['author_org', 'authors_a', 'orgs_a', 'authors_b', 'orgs_b']]\n",
    "tmp['authors_a']=tmp['authors_a'].apply(lambda x: [] if not isinstance(x,list) else x)\n",
    "tmp['orgs_a']=tmp['orgs_a'].apply(lambda x:[] if not isinstance(x,list) else x)\n",
    "tmp['authors_b']=tmp['authors_b'].apply(lambda x:[] if not isinstance(x,list) else x)\n",
    "tmp['orgs_b']=tmp['orgs_b'].apply(lambda x:[] if not isinstance(x,list) else x)\n",
    "def func(a, b):\n",
    "    cnt = 0\n",
    "    for x in b:\n",
    "        for y in x:\n",
    "            if a == y:\n",
    "                cnt += 1\n",
    "    return cnt\n",
    "\n",
    "# %%time\n",
    "tmp['author_org_in_orgs_b_times'] = tmp.apply(lambda row: func(row['author_org'], row['orgs_b']), axis=1)\n",
    "\n",
    "def func(a, b):\n",
    "    b = set([y for x in b for y in x])\n",
    "    # b = set([x for x in b])\n",
    "    a = set(a)\n",
    "    return len(a & b)\n",
    "print(82*'-')\n",
    "# %%time\n",
    "tmp['author_interset_num'] = tmp.apply(lambda row: func(row['authors_a'], row['authors_b']), axis=1)\n",
    "\n",
    "tmp['author_interset_num/paper_ids_len'] = (tmp['author_interset_num'] / (data['paper_ids_len'] - (data['label'] == 1).astype(int))).fillna(0)\n",
    "\n",
    "tmp.head()\n",
    "\n",
    "tmp['author_interset_num'].value_counts()\n",
    "\n",
    "tmp[['author_org_in_orgs_b_times', 'author_interset_num', 'author_interset_num/paper_ids_len']].to_pickle('./feat/tmp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================切换目录=====================================\n",
      "/home/shichaoyu/perl5/IRpro2\n",
      "/home/shichaoyu/perl5/IRpro2/dataset\n",
      "['train', 'cat_feature_index.3c4c9673-11b0f6ac-cb85fb96-91833486.tmp', 'catboost_info', 'cat_feature_index.40b30d8b-f30e5088-cc5f7d50-21dddf23.tmp', 'out', 'cna_test_data', 'cna_data', 'pkl2', 'cat_feature_index.45ed2826-847e7041-c540e66c-339d9b99.tmp', 'cat_feature_index.3be0c692-8a8fe0c8-bb8954c3-56fb0ff.tmp', 'feat']\n",
      "(3142238, 5)\n",
      "(505765, 4)\n",
      "(3648003, 5)\n",
      "15\n",
      "['year_a', 'year_b_min', 'year_b_max', 'year_b_mean', 'year_b_std', 'year_b_mm2', 'year_b_min-year_a', 'year_b_max-year_a', 'year_b_mean-year_a', 'year_b_mm2-year_a', 'year_inside_range', 'year_b_ai2', 'author_org_in_orgs_b_times', 'author_interset_num', 'author_interset_num/paper_ids_len']\n",
      "220\n"
     ]
    }
   ],
   "source": [
    "# !pip install catboost\n",
    "import os\n",
    "print(\"=================================切换目录=====================================\")\n",
    "print(os.getcwd()) # 打印当前工作目录\n",
    "os.chdir('/home/shichaoyu/perl5/IRpro2/dataset/')\n",
    "print(os.getcwd())\n",
    "print(os.listdir())\n",
    "\n",
    "## 7 baseline_v1 模型训练预测，结果生成\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "\n",
    "out_dir = './out/'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "train_data = pd.read_pickle('./pkl2/train_data.pkl')\n",
    "test_data = pd.read_pickle('./pkl2/valid_data.pkl')\n",
    "\n",
    "data = pd.concat([train_data, test_data]).reset_index(drop=True)\n",
    "\n",
    "data.head()\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(data.shape)\n",
    "\n",
    "##\n",
    "time_feat_a = pd.read_pickle('./feat/time_feat_a.pkl')\n",
    "tmp = pd.read_pickle('./feat/tmp.pkl')\n",
    "\n",
    "feats = [time_feat_a, tmp]\n",
    "\n",
    "data = pd.concat([data] + feats, axis=1)\n",
    "\n",
    "data.head()\n",
    "\n",
    "drop_feat = ['author_id', 'author_name', 'author_org', 'paper_id', 'label']\n",
    "used_feat = [c for c in data.columns if c not in drop_feat]\n",
    "print(len(used_feat))\n",
    "print(used_feat)\n",
    "\n",
    "train = data[:len(train_data)]\n",
    "test = data[len(train_data):]\n",
    "\n",
    "test_x = test[used_feat]\n",
    "\n",
    "# cv split according to author names\n",
    "train_author_name = train['author_name'].unique()\n",
    "print(len(train_author_name))\n",
    "\n",
    "def gen_dict(df, label):\n",
    "    df = df[['paper_id', 'author_name', 'author_id', label]]\n",
    "    res = df.groupby(['paper_id', 'author_name'])[label].apply(np.argmax).reset_index()\n",
    "    res.columns = ['paper_id', 'author_name', 'index']\n",
    "    idx_name = df[['author_id']].reset_index()\n",
    "    res = res.merge(idx_name, 'left', 'index')\n",
    "    from collections import defaultdict\n",
    "    res_dict = defaultdict(list)\n",
    "    for pid, aid in res[['paper_id', 'author_id']].values:\n",
    "        res_dict[aid].append(pid)\n",
    "    return res_dict\n",
    "\n",
    "def f1_score(pred_dict, true_dict):\n",
    "    total_unassigned_paper = np.sum([len(l) for l in true_dict.values()])\n",
    "    print('total_unassigned_paper: ', total_unassigned_paper)\n",
    "    print('true author num: ', len(true_dict))\n",
    "    author_weight = dict((k, len(v) / total_unassigned_paper) for k, v in true_dict.items())\n",
    "    author_precision = {}\n",
    "    author_recall = {}\n",
    "    for author in author_weight.keys():\n",
    "        # total pred, total belong, correct pred\n",
    "        total_belong = len(true_dict[author])\n",
    "        total_pred = (len(pred_dict[author]) if author in pred_dict else 0)\n",
    "        correct_pred = len(set(true_dict[author]) & (set(pred_dict[author]) if author in pred_dict else set()))\n",
    "        author_precision[author] = (correct_pred/total_pred) if total_pred > 0 else 0\n",
    "        author_recall[author] = correct_pred / total_belong\n",
    "        \n",
    "    weighted_precision = 0\n",
    "    weighted_recall = 0\n",
    "    for author, weight in author_weight.items():\n",
    "        weighted_precision += weight * author_precision[author]\n",
    "        weighted_recall += weight * author_recall[author]\n",
    "    weighted_f1 = 2 * weighted_precision * weighted_recall / (weighted_precision + weighted_recall)\n",
    "    print('weighted_precision: %f, weighted_recall: %f, weighted_f1: %f' %(weighted_precision, weighted_recall, weighted_f1))\n",
    "    return weighted_precision, weighted_recall, weighted_f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  提交日志\n",
    "\n",
    "#### n=10\n",
    "1. **0.619934746675664**  fold 10 round 1464 : auc: 0.982882 | mean auc 0.985608 | F1: 0.944590 | mean F1: 0.947347   n=10\n",
    "      iterations=10000,  learning_rate=0.08, early_stopping_rounds=300,loss_function='CrossEntropy', \n",
    "      \n",
    "2. **0.620929218125254** fold 10 round 1594 : auc: 0.982975 | mean auc 0.985650 | F1: 0.945342 | mean F1: 0.948062  \n",
    "      iterations=10000,  learning_rate=0.08, **early_stopping_rounds=400**,loss_function='CrossEntropy',   n=10\n",
    "      \n",
    "3. **0.621775397186683**  fold 10 round 1341 : auc: 0.983031 | mean auc 0.985643 | F1: 0.943900 | mean F1: 0.947486\n",
    "      **iterations=15000**,  learning_rate=0.08, early_stopping_rounds=300,loss_function='CrossEntropy',   n=10\n",
    "     \n",
    "4. **0.621081510328536**  fold 10 round 1359 : auc: 0.982904 | mean auc 0.985647 | F1: 0.944841 | mean F1: 0.947348\n",
    "      **iterations=16000**,  learning_rate=0.08, early_stopping_rounds=300,loss_function='CrossEntropy',   n=10   \n",
    "\n",
    "5. **0.622136901630442**  fold 10 round 998 : auc: 0.982874 | mean auc 0.985600 | F1: 0.944098 | mean F1: 0.947723 \n",
    "      iterations=10000,  learning_rate=0.08,**early_stopping_rounds=500**,loss_function='CrossEntropy',   n=10 \n",
    "\n",
    "6. **0.621305163601349** fold 10 round 983 : auc: 0.982874 | mean auc 0.985610 | F1: 0.945187 | mean F1: 0.948160 \n",
    "      iterations=10000,  learning_rate=0.08,**early_stopping_rounds=550**,loss_function='CrossEntropy',   n=10 \n",
    "      \n",
    " ===============================================================================================\n",
    " \n",
    "6. **0.620987876156855** \n",
    "      iterations=12000,  learning_rate=0.08,**early_stopping_rounds=300**,loss_function='CrossEntropy',   n=10 \n",
    "      \n",
    "#### n=5\n",
    "\n",
    "1. **0.620902879499788**  fold 10 round 816 : auc: 0.982305 | mean auc 0.985375 | F1: 0.956018 | mean F1: 0.959864\n",
    "      iterations=10000,  learning_rate=0.08, early_stopping_rounds=300,loss_function='CrossEntropy',   n=5\n",
    "\n",
    "2. **0.620202428314349**  fold 10 round 1048 : auc: 0.982160 | mean auc 0.985361 | F1: 0.955270 | mean F1: 0.960090\n",
    "      iterations=15000,  learning_rate=0.08, early_stopping_rounds=300,loss_function='CrossEntropy',   n=5\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "******************************\n",
      "******************************\n"
     ]
    },
    {
     "ename": "CatBoostError",
     "evalue": "catboost/cuda/cuda_lib/cuda_manager.cpp:201: Condition violated: `State == nullptr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ec3f1ef5f289>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#                             depth=8,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                            )\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mcbt_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;31m#         with open('./models/fold%d_cbt_v1.mdl' % index, 'wb') as file:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#             pickle.dump(cbt_model, file)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, text_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[1;32m   3791\u001b[0m         self._fit(X, y, cat_features, text_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n\u001b[1;32m   3792\u001b[0m                   \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3793\u001b[0;31m                   silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\n\u001b[0m\u001b[1;32m   3794\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, text_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[1;32m   1686\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                 \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m                 \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"init_model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m             )\n\u001b[1;32m   1690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCatBoostError\u001b[0m: catboost/cuda/cuda_lib/cuda_manager.cpp:201: Condition violated: `State == nullptr'"
     ]
    }
   ],
   "source": [
    "# 8. 参数调整\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] ='3,4,5'\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "preds = np.zeros((test.shape[0], 2))\n",
    "scores = []\n",
    "f1_scores = []\n",
    "has_saved = False\n",
    "imp = pd.DataFrame()\n",
    "imp['feat'] = used_feat\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for index, (tr_idx, va_idx) in enumerate(kfold.split(train_author_name)):\n",
    "    print('*' * 30)\n",
    "#     验证集，测试集索引  同时得到对应的数据集，进行交叉验证\n",
    "    trn_aname, val_aname = train_author_name[tr_idx], train_author_name[va_idx]\n",
    "    trn_dat = train[train['author_name'].isin(trn_aname)]\n",
    "    val_dat = train[train['author_name'].isin(val_aname)]\n",
    "    print('*' * 30)\n",
    "    X_train, y_train, X_valid, y_valid = trn_dat[used_feat], trn_dat['label'], val_dat[used_feat], val_dat['label']\n",
    "    cate_features = []\n",
    "    train_pool = Pool(X_train, y_train, cat_features=cate_features)\n",
    "    eval_pool = Pool(X_valid, y_valid,cat_features=cate_features)\n",
    "    print('*' * 30)\n",
    "    if not has_saved: \n",
    "        cbt_model = CatBoostClassifier(iterations=21000,\n",
    "                           learning_rate=0.08,\n",
    "                           eval_metric='AUC',\n",
    "                           use_best_model=True,\n",
    "#                            leaf_estimation_method='Newton',\n",
    "                           random_seed=42,\n",
    "                           logging_level='Verbose',\n",
    "                           task_type='GPU',\n",
    "                           devices='5',\n",
    "                           gpu_ram_part=0.5,\n",
    "                           early_stopping_rounds=300,\n",
    "                           loss_function='Logloss',\n",
    "#                             depth=8,\n",
    "                           )\n",
    "        cbt_model.fit(train_pool, eval_set=eval_pool, verbose=100)\n",
    "#         with open('./models/fold%d_cbt_v1.mdl' % index, 'wb') as file:\n",
    "#             pickle.dump(cbt_model, file)\n",
    "#     else:\n",
    "#         with open('./models/fold%d_cbt_v1.mdl' % index, 'rb') as file:\n",
    "#             cbt_model = pickle.load(file)\n",
    "    \n",
    "    imp['score%d' % (index+1)] = cbt_model.feature_importances_\n",
    "    \n",
    "    val_dat['pred'] = cbt_model.predict_proba(X_valid)[:, 1]\n",
    "    val_pred_dict = gen_dict(val_dat, 'pred')\n",
    "    val_true_dict = gen_dict(val_dat, 'label')\n",
    "    precision, recall, f1 = f1_score(val_pred_dict, val_true_dict)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    score = cbt_model.best_score_['validation']['AUC']\n",
    "    scores.append(score)\n",
    "    print('fold %d round %d : auc: %.6f | mean auc %.6f | F1: %.6f | mean F1: %.6f' % (index+1, cbt_model.best_iteration_, score,np.mean(scores), f1, np.mean(f1_scores))) \n",
    "    preds += cbt_model.predict_proba(test_x)  \n",
    "#     break\n",
    "    del cbt_model, train_pool, eval_pool\n",
    "    del X_train, y_train, X_valid, y_valid\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "#     mdls.append(cbt_model)\n",
    "\n",
    "imp.sort_values(by='score1', ascending=False)\n",
    "\n",
    "test_data['pred'] = preds[:, 1]\n",
    "\n",
    "test_data.head()\n",
    "\n",
    "result_dict = gen_dict(test_data, 'pred')\n",
    "\n",
    "len(result_dict)\n",
    "\n",
    "import json\n",
    "import time\n",
    "localtime = time.localtime(time.time())\n",
    "save_path = './out/result_%02d%02d%02d%02d%02d.json' % (localtime[1], localtime[2], localtime[3], localtime[4],localtime[5])\n",
    "with open(save_path, 'w') as file:\n",
    "    file.write(json.dumps(result_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "******************************\n",
      "******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 3639.5625 Total: 10989.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.9741388\ttest: 0.9733723\tbest: 0.9733723 (0)\ttotal: 127ms\tremaining: 2m 6s\n",
      "100:\tlearn: 0.9847890\ttest: 0.9841924\tbest: 0.9841927 (99)\ttotal: 8.13s\tremaining: 1m 12s\n",
      "200:\tlearn: 0.9858270\ttest: 0.9850225\tbest: 0.9850236 (199)\ttotal: 16.4s\tremaining: 1m 5s\n",
      "300:\tlearn: 0.9863668\ttest: 0.9852910\tbest: 0.9852929 (299)\ttotal: 24.8s\tremaining: 57.6s\n",
      "400:\tlearn: 0.9867468\ttest: 0.9854158\tbest: 0.9854159 (399)\ttotal: 33.3s\tremaining: 49.8s\n",
      "500:\tlearn: 0.9870386\ttest: 0.9854746\tbest: 0.9854746 (500)\ttotal: 41.6s\tremaining: 41.5s\n",
      "600:\tlearn: 0.9872616\ttest: 0.9855222\tbest: 0.9855261 (596)\ttotal: 49.8s\tremaining: 33.1s\n",
      "700:\tlearn: 0.9874696\ttest: 0.9854851\tbest: 0.9855261 (596)\ttotal: 58.1s\tremaining: 24.8s\n",
      "800:\tlearn: 0.9876445\ttest: 0.9855247\tbest: 0.9855261 (596)\ttotal: 1m 6s\tremaining: 16.5s\n",
      "900:\tlearn: 0.9878028\ttest: 0.9855166\tbest: 0.9855345 (822)\ttotal: 1m 14s\tremaining: 8.16s\n",
      "999:\tlearn: 0.9879828\ttest: 0.9855111\tbest: 0.9855509 (930)\ttotal: 1m 22s\tremaining: 0us\n",
      "bestTest = 0.9855508804\n",
      "bestIteration = 930\n",
      "Shrink model to first 931 iterations.\n",
      "total_unassigned_paper:  17182\n",
      "true author num:  1973\n",
      "weighted_precision: 0.949424, weighted_recall: 0.950064, weighted_f1: 0.949744\n",
      "fold 1 round 930 : auc: 0.985551 | mean auc 0.985551 | F1: 0.949744 | mean F1: 0.949744\n",
      "******************************\n",
      "******************************\n",
      "******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 3639.5625 Total: 10989.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.9747756\ttest: 0.9708628\tbest: 0.9708628 (0)\ttotal: 20.7ms\tremaining: 20.7s\n",
      "100:\tlearn: 0.9847887\ttest: 0.9829091\tbest: 0.9829091 (100)\ttotal: 7.99s\tremaining: 1m 11s\n",
      "200:\tlearn: 0.9858344\ttest: 0.9837517\tbest: 0.9837517 (200)\ttotal: 16.2s\tremaining: 1m 4s\n",
      "300:\tlearn: 0.9864483\ttest: 0.9841740\tbest: 0.9841811 (299)\ttotal: 24.1s\tremaining: 55.9s\n",
      "400:\tlearn: 0.9867832\ttest: 0.9844134\tbest: 0.9844134 (400)\ttotal: 32s\tremaining: 47.8s\n",
      "500:\tlearn: 0.9870788\ttest: 0.9846603\tbest: 0.9846805 (489)\ttotal: 40s\tremaining: 39.9s\n",
      "600:\tlearn: 0.9873474\ttest: 0.9848300\tbest: 0.9848300 (600)\ttotal: 48.2s\tremaining: 32s\n",
      "700:\tlearn: 0.9875251\ttest: 0.9848728\tbest: 0.9848739 (698)\ttotal: 56.4s\tremaining: 24.1s\n",
      "800:\tlearn: 0.9876844\ttest: 0.9850156\tbest: 0.9850158 (798)\ttotal: 1m 4s\tremaining: 16.1s\n",
      "900:\tlearn: 0.9878623\ttest: 0.9850357\tbest: 0.9850537 (891)\ttotal: 1m 13s\tremaining: 8.06s\n",
      "999:\tlearn: 0.9880017\ttest: 0.9849695\tbest: 0.9850537 (891)\ttotal: 1m 21s\tremaining: 0us\n",
      "bestTest = 0.9850537479\n",
      "bestIteration = 891\n",
      "Shrink model to first 892 iterations.\n",
      "total_unassigned_paper:  11311\n",
      "true author num:  1010\n",
      "weighted_precision: 0.946157, weighted_recall: 0.942269, weighted_f1: 0.944209\n",
      "fold 2 round 891 : auc: 0.985054 | mean auc 0.985302 | F1: 0.944209 | mean F1: 0.946976\n",
      "******************************\n",
      "******************************\n",
      "******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 3639.5625 Total: 10989.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.9740846\ttest: 0.9740320\tbest: 0.9740320 (0)\ttotal: 222ms\tremaining: 3m 41s\n",
      "100:\tlearn: 0.9849703\ttest: 0.9839569\tbest: 0.9839569 (100)\ttotal: 8.53s\tremaining: 1m 15s\n",
      "200:\tlearn: 0.9860473\ttest: 0.9846439\tbest: 0.9846458 (199)\ttotal: 16.8s\tremaining: 1m 6s\n",
      "300:\tlearn: 0.9865253\ttest: 0.9848561\tbest: 0.9848569 (299)\ttotal: 24.9s\tremaining: 57.8s\n",
      "400:\tlearn: 0.9868927\ttest: 0.9849120\tbest: 0.9849443 (368)\ttotal: 33.2s\tremaining: 49.5s\n",
      "500:\tlearn: 0.9871950\ttest: 0.9849870\tbest: 0.9849870 (500)\ttotal: 41.6s\tremaining: 41.5s\n",
      "600:\tlearn: 0.9874961\ttest: 0.9850749\tbest: 0.9850749 (600)\ttotal: 49.8s\tremaining: 33.1s\n",
      "700:\tlearn: 0.9876885\ttest: 0.9850677\tbest: 0.9850819 (610)\ttotal: 58.3s\tremaining: 24.9s\n",
      "800:\tlearn: 0.9878553\ttest: 0.9851208\tbest: 0.9851221 (799)\ttotal: 1m 6s\tremaining: 16.6s\n",
      "900:\tlearn: 0.9880077\ttest: 0.9851296\tbest: 0.9851393 (885)\ttotal: 1m 14s\tremaining: 8.23s\n",
      "999:\tlearn: 0.9881728\ttest: 0.9851024\tbest: 0.9851393 (885)\ttotal: 1m 22s\tremaining: 0us\n",
      "bestTest = 0.9851393402\n",
      "bestIteration = 885\n",
      "Shrink model to first 886 iterations.\n",
      "total_unassigned_paper:  27481\n",
      "true author num:  3534\n",
      "weighted_precision: 0.950345, weighted_recall: 0.951748, weighted_f1: 0.951046\n",
      "fold 3 round 885 : auc: 0.985139 | mean auc 0.985248 | F1: 0.951046 | mean F1: 0.948333\n",
      "******************************\n",
      "******************************\n",
      "******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 3639.5625 Total: 10989.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.9732037\ttest: 0.9719571\tbest: 0.9719571 (0)\ttotal: 155ms\tremaining: 2m 35s\n",
      "100:\tlearn: 0.9846037\ttest: 0.9846744\tbest: 0.9846744 (100)\ttotal: 7.91s\tremaining: 1m 10s\n",
      "200:\tlearn: 0.9856971\ttest: 0.9854196\tbest: 0.9854196 (200)\ttotal: 15.8s\tremaining: 1m 2s\n",
      "300:\tlearn: 0.9862596\ttest: 0.9857939\tbest: 0.9857939 (300)\ttotal: 24.1s\tremaining: 56s\n",
      "400:\tlearn: 0.9867012\ttest: 0.9859315\tbest: 0.9859357 (394)\ttotal: 32.3s\tremaining: 48.2s\n",
      "500:\tlearn: 0.9870234\ttest: 0.9860500\tbest: 0.9860541 (497)\ttotal: 40.4s\tremaining: 40.3s\n",
      "600:\tlearn: 0.9872608\ttest: 0.9861086\tbest: 0.9861246 (565)\ttotal: 48.3s\tremaining: 32.1s\n",
      "700:\tlearn: 0.9874603\ttest: 0.9861423\tbest: 0.9861441 (691)\ttotal: 56.7s\tremaining: 24.2s\n",
      "800:\tlearn: 0.9876494\ttest: 0.9862029\tbest: 0.9862089 (792)\ttotal: 1m 4s\tremaining: 16.1s\n",
      "900:\tlearn: 0.9877899\ttest: 0.9862325\tbest: 0.9862334 (899)\ttotal: 1m 13s\tremaining: 8.09s\n",
      "999:\tlearn: 0.9879458\ttest: 0.9862536\tbest: 0.9862556 (993)\ttotal: 1m 22s\tremaining: 0us\n",
      "bestTest = 0.9862555563\n",
      "bestIteration = 993\n",
      "Shrink model to first 994 iterations.\n",
      "total_unassigned_paper:  22023\n",
      "true author num:  1968\n",
      "weighted_precision: 0.950727, weighted_recall: 0.944149, weighted_f1: 0.947427\n",
      "fold 4 round 993 : auc: 0.986256 | mean auc 0.985500 | F1: 0.947427 | mean F1: 0.948106\n",
      "******************************\n",
      "******************************\n",
      "******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 3639.5625 Total: 10989.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.9738021\ttest: 0.9786137\tbest: 0.9786137 (0)\ttotal: 179ms\tremaining: 2m 58s\n",
      "100:\tlearn: 0.9845566\ttest: 0.9868599\tbest: 0.9868599 (100)\ttotal: 8.41s\tremaining: 1m 14s\n",
      "200:\tlearn: 0.9855628\ttest: 0.9875995\tbest: 0.9875995 (200)\ttotal: 16.3s\tremaining: 1m 4s\n",
      "300:\tlearn: 0.9861210\ttest: 0.9879251\tbest: 0.9879251 (300)\ttotal: 24.8s\tremaining: 57.5s\n",
      "400:\tlearn: 0.9865218\ttest: 0.9880518\tbest: 0.9880518 (400)\ttotal: 33.1s\tremaining: 49.5s\n",
      "500:\tlearn: 0.9868643\ttest: 0.9881657\tbest: 0.9881673 (498)\ttotal: 41.7s\tremaining: 41.5s\n",
      "600:\tlearn: 0.9871381\ttest: 0.9881925\tbest: 0.9881991 (588)\ttotal: 50s\tremaining: 33.2s\n",
      "700:\tlearn: 0.9873548\ttest: 0.9881843\tbest: 0.9881991 (588)\ttotal: 59s\tremaining: 25.2s\n",
      "800:\tlearn: 0.9875316\ttest: 0.9881918\tbest: 0.9881991 (588)\ttotal: 1m 7s\tremaining: 16.8s\n",
      "900:\tlearn: 0.9876939\ttest: 0.9881798\tbest: 0.9881991 (588)\ttotal: 1m 15s\tremaining: 8.34s\n",
      "999:\tlearn: 0.9878488\ttest: 0.9881678\tbest: 0.9881991 (588)\ttotal: 1m 24s\tremaining: 0us\n",
      "bestTest = 0.9881991148\n",
      "bestIteration = 588\n",
      "Shrink model to first 589 iterations.\n",
      "total_unassigned_paper:  17814\n",
      "true author num:  2062\n",
      "weighted_precision: 0.956534, weighted_recall: 0.956439, weighted_f1: 0.956487\n",
      "fold 5 round 588 : auc: 0.988199 | mean auc 0.986040 | F1: 0.956487 | mean F1: 0.949783\n",
      "******************************\n",
      "******************************\n",
      "******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 3639.5625 Total: 10989.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.9741390\ttest: 0.9717250\tbest: 0.9717250 (0)\ttotal: 139ms\tremaining: 2m 18s\n",
      "100:\tlearn: 0.9850613\ttest: 0.9821891\tbest: 0.9821891 (100)\ttotal: 8.27s\tremaining: 1m 13s\n",
      "200:\tlearn: 0.9860506\ttest: 0.9831073\tbest: 0.9831170 (195)\ttotal: 16.4s\tremaining: 1m 5s\n",
      "300:\tlearn: 0.9865788\ttest: 0.9834190\tbest: 0.9834218 (293)\ttotal: 24.7s\tremaining: 57.4s\n",
      "400:\tlearn: 0.9870408\ttest: 0.9836527\tbest: 0.9836527 (400)\ttotal: 33s\tremaining: 49.3s\n",
      "500:\tlearn: 0.9873446\ttest: 0.9836993\tbest: 0.9837013 (481)\ttotal: 40.9s\tremaining: 40.7s\n",
      "600:\tlearn: 0.9875663\ttest: 0.9836786\tbest: 0.9837096 (512)\ttotal: 49s\tremaining: 32.5s\n",
      "700:\tlearn: 0.9878179\ttest: 0.9837478\tbest: 0.9837685 (688)\ttotal: 57.6s\tremaining: 24.6s\n",
      "800:\tlearn: 0.9879855\ttest: 0.9837441\tbest: 0.9837685 (688)\ttotal: 1m 6s\tremaining: 16.4s\n",
      "900:\tlearn: 0.9881680\ttest: 0.9837105\tbest: 0.9837685 (688)\ttotal: 1m 14s\tremaining: 8.22s\n",
      "999:\tlearn: 0.9883212\ttest: 0.9837069\tbest: 0.9837685 (688)\ttotal: 1m 23s\tremaining: 0us\n",
      "bestTest = 0.9837684929\n",
      "bestIteration = 688\n",
      "Shrink model to first 689 iterations.\n",
      "total_unassigned_paper:  25019\n",
      "true author num:  2517\n",
      "weighted_precision: 0.947535, weighted_recall: 0.944362, weighted_f1: 0.945946\n",
      "fold 6 round 688 : auc: 0.983768 | mean auc 0.985661 | F1: 0.945946 | mean F1: 0.949143\n",
      "******************************\n",
      "******************************\n",
      "******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 3639.5625 Total: 10989.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.9741672\ttest: 0.9732900\tbest: 0.9732900 (0)\ttotal: 64.3ms\tremaining: 1m 4s\n",
      "100:\tlearn: 0.9848627\ttest: 0.9846819\tbest: 0.9846819 (100)\ttotal: 7.94s\tremaining: 1m 10s\n",
      "200:\tlearn: 0.9859210\ttest: 0.9853123\tbest: 0.9853123 (200)\ttotal: 16.4s\tremaining: 1m 5s\n",
      "300:\tlearn: 0.9863819\ttest: 0.9855202\tbest: 0.9855202 (300)\ttotal: 24.8s\tremaining: 57.6s\n",
      "400:\tlearn: 0.9867551\ttest: 0.9856734\tbest: 0.9856739 (398)\ttotal: 33.1s\tremaining: 49.5s\n",
      "500:\tlearn: 0.9870594\ttest: 0.9857694\tbest: 0.9857694 (500)\ttotal: 41.1s\tremaining: 40.9s\n",
      "600:\tlearn: 0.9873326\ttest: 0.9858515\tbest: 0.9858519 (598)\ttotal: 49.3s\tremaining: 32.7s\n",
      "700:\tlearn: 0.9875487\ttest: 0.9858585\tbest: 0.9858631 (662)\ttotal: 57.5s\tremaining: 24.5s\n",
      "800:\tlearn: 0.9877342\ttest: 0.9858438\tbest: 0.9858631 (662)\ttotal: 1m 5s\tremaining: 16.4s\n",
      "900:\tlearn: 0.9878948\ttest: 0.9858500\tbest: 0.9858631 (662)\ttotal: 1m 13s\tremaining: 8.13s\n",
      "999:\tlearn: 0.9880409\ttest: 0.9858836\tbest: 0.9858858 (996)\ttotal: 1m 21s\tremaining: 0us\n",
      "bestTest = 0.9858857691\n",
      "bestIteration = 996\n",
      "Shrink model to first 997 iterations.\n",
      "total_unassigned_paper:  24321\n",
      "true author num:  2886\n",
      "weighted_precision: 0.942762, weighted_recall: 0.942478, weighted_f1: 0.942620\n",
      "fold 7 round 996 : auc: 0.985886 | mean auc 0.985693 | F1: 0.942620 | mean F1: 0.948211\n",
      "******************************\n",
      "******************************\n",
      "******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 3639.5625 Total: 10989.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.9736023\ttest: 0.9771281\tbest: 0.9771281 (0)\ttotal: 156ms\tremaining: 2m 36s\n",
      "100:\tlearn: 0.9845552\ttest: 0.9859419\tbest: 0.9859566 (99)\ttotal: 8.18s\tremaining: 1m 12s\n",
      "200:\tlearn: 0.9856108\ttest: 0.9867259\tbest: 0.9867288 (195)\ttotal: 15.8s\tremaining: 1m 2s\n",
      "300:\tlearn: 0.9861561\ttest: 0.9869714\tbest: 0.9869768 (298)\ttotal: 24.2s\tremaining: 56.3s\n",
      "400:\tlearn: 0.9865126\ttest: 0.9871359\tbest: 0.9871359 (400)\ttotal: 32s\tremaining: 47.9s\n",
      "500:\tlearn: 0.9868208\ttest: 0.9872388\tbest: 0.9872441 (490)\ttotal: 40.2s\tremaining: 40.1s\n",
      "600:\tlearn: 0.9870730\ttest: 0.9873314\tbest: 0.9873491 (589)\ttotal: 48.4s\tremaining: 32.1s\n",
      "700:\tlearn: 0.9872912\ttest: 0.9873912\tbest: 0.9873988 (692)\ttotal: 56.6s\tremaining: 24.1s\n",
      "800:\tlearn: 0.9874997\ttest: 0.9873504\tbest: 0.9873988 (692)\ttotal: 1m 5s\tremaining: 16.2s\n",
      "900:\tlearn: 0.9876644\ttest: 0.9873019\tbest: 0.9873988 (692)\ttotal: 1m 13s\tremaining: 8.08s\n",
      "999:\tlearn: 0.9878344\ttest: 0.9873140\tbest: 0.9873988 (692)\ttotal: 1m 21s\tremaining: 0us\n",
      "bestTest = 0.987398833\n",
      "bestIteration = 692\n",
      "Shrink model to first 693 iterations.\n",
      "total_unassigned_paper:  24152\n",
      "true author num:  2658\n",
      "weighted_precision: 0.947469, weighted_recall: 0.946547, weighted_f1: 0.947008\n",
      "fold 8 round 692 : auc: 0.987399 | mean auc 0.985906 | F1: 0.947008 | mean F1: 0.948061\n",
      "******************************\n",
      "******************************\n",
      "******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 3639.5625 Total: 10989.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.9743982\ttest: 0.9753079\tbest: 0.9753079 (0)\ttotal: 84.1ms\tremaining: 1m 24s\n",
      "100:\tlearn: 0.9848181\ttest: 0.9849758\tbest: 0.9849758 (100)\ttotal: 8.47s\tremaining: 1m 15s\n",
      "200:\tlearn: 0.9857537\ttest: 0.9853680\tbest: 0.9853697 (196)\ttotal: 16.2s\tremaining: 1m 4s\n",
      "300:\tlearn: 0.9863255\ttest: 0.9856862\tbest: 0.9856902 (299)\ttotal: 24.4s\tremaining: 56.6s\n",
      "400:\tlearn: 0.9867280\ttest: 0.9858955\tbest: 0.9858961 (399)\ttotal: 32.8s\tremaining: 48.9s\n",
      "500:\tlearn: 0.9870069\ttest: 0.9859556\tbest: 0.9859575 (499)\ttotal: 41.2s\tremaining: 41s\n",
      "600:\tlearn: 0.9872673\ttest: 0.9859295\tbest: 0.9859751 (514)\ttotal: 49.6s\tremaining: 32.9s\n",
      "700:\tlearn: 0.9874999\ttest: 0.9859340\tbest: 0.9859751 (514)\ttotal: 58s\tremaining: 24.7s\n",
      "800:\tlearn: 0.9876391\ttest: 0.9859482\tbest: 0.9859751 (514)\ttotal: 1m 6s\tremaining: 16.5s\n",
      "900:\tlearn: 0.9878050\ttest: 0.9858979\tbest: 0.9859751 (514)\ttotal: 1m 14s\tremaining: 8.22s\n",
      "999:\tlearn: 0.9879754\ttest: 0.9859595\tbest: 0.9859751 (514)\ttotal: 1m 22s\tremaining: 0us\n",
      "bestTest = 0.9859750867\n",
      "bestIteration = 514\n",
      "Shrink model to first 515 iterations.\n",
      "total_unassigned_paper:  19020\n",
      "true author num:  2434\n",
      "weighted_precision: 0.952008, weighted_recall: 0.951840, weighted_f1: 0.951924\n",
      "fold 9 round 514 : auc: 0.985975 | mean auc 0.985914 | F1: 0.951924 | mean F1: 0.948490\n",
      "******************************\n",
      "******************************\n",
      "******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 3639.5625 Total: 10989.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.9745938\ttest: 0.9692506\tbest: 0.9692506 (0)\ttotal: 108ms\tremaining: 1m 47s\n",
      "100:\tlearn: 0.9850067\ttest: 0.9809700\tbest: 0.9809700 (100)\ttotal: 8.19s\tremaining: 1m 12s\n",
      "200:\tlearn: 0.9860573\ttest: 0.9819126\tbest: 0.9819126 (200)\ttotal: 16.7s\tremaining: 1m 6s\n",
      "300:\tlearn: 0.9865771\ttest: 0.9822334\tbest: 0.9822427 (293)\ttotal: 25.3s\tremaining: 58.7s\n",
      "400:\tlearn: 0.9869813\ttest: 0.9824619\tbest: 0.9824634 (395)\ttotal: 33.6s\tremaining: 50.2s\n",
      "500:\tlearn: 0.9872648\ttest: 0.9826027\tbest: 0.9826184 (486)\ttotal: 42s\tremaining: 41.8s\n",
      "600:\tlearn: 0.9874830\ttest: 0.9826946\tbest: 0.9827181 (589)\ttotal: 49.9s\tremaining: 33.1s\n",
      "700:\tlearn: 0.9876997\ttest: 0.9827538\tbest: 0.9827698 (686)\ttotal: 58.1s\tremaining: 24.8s\n",
      "800:\tlearn: 0.9878742\ttest: 0.9828046\tbest: 0.9828047 (790)\ttotal: 1m 6s\tremaining: 16.6s\n",
      "900:\tlearn: 0.9880486\ttest: 0.9828023\tbest: 0.9828233 (862)\ttotal: 1m 14s\tremaining: 8.23s\n",
      "999:\tlearn: 0.9881766\ttest: 0.9828618\tbest: 0.9828739 (983)\ttotal: 1m 23s\tremaining: 0us\n",
      "bestTest = 0.982873857\n",
      "bestIteration = 983\n",
      "Shrink model to first 984 iterations.\n",
      "total_unassigned_paper:  17093\n",
      "true author num:  1779\n",
      "weighted_precision: 0.945778, weighted_recall: 0.944597, weighted_f1: 0.945187\n",
      "fold 10 round 983 : auc: 0.982874 | mean auc 0.985610 | F1: 0.945187 | mean F1: 0.948160\n"
     ]
    }
   ],
   "source": [
    "# 8. 参数调整\n",
    "from sklearn.model_selection import KFold\n",
    "preds = np.zeros((test.shape[0], 2))\n",
    "scores = []\n",
    "f1_scores = []\n",
    "has_saved = False\n",
    "imp = pd.DataFrame()\n",
    "imp['feat'] = used_feat\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for index, (tr_idx, va_idx) in enumerate(kfold.split(train_author_name)):\n",
    "    print('*' * 30)\n",
    "#     验证集，测试集索引  同时得到对应的数据集，进行交叉验证\n",
    "    trn_aname, val_aname = train_author_name[tr_idx], train_author_name[va_idx]\n",
    "    trn_dat = train[train['author_name'].isin(trn_aname)]\n",
    "    val_dat = train[train['author_name'].isin(val_aname)]\n",
    "    print('*' * 30)\n",
    "    X_train, y_train, X_valid, y_valid = trn_dat[used_feat], trn_dat['label'], val_dat[used_feat], val_dat['label']\n",
    "    cate_features = []\n",
    "    train_pool = Pool(X_train, y_train, cat_features=cate_features)\n",
    "    eval_pool = Pool(X_valid, y_valid,cat_features=cate_features)\n",
    "    print('*' * 30)\n",
    "    if not has_saved: \n",
    "        cbt_model = CatBoostClassifier(iterations=1000,\n",
    "                           learning_rate=0.08,\n",
    "                           eval_metric='AUC',\n",
    "                           use_best_model=True,\n",
    "#                            leaf_estimation_method='Newton',\n",
    "                           random_seed=42,\n",
    "                           logging_level='Verbose',\n",
    "                           task_type='GPU',\n",
    "                           devices=[1,2],\n",
    "                           gpu_ram_part=0.5,\n",
    "                           early_stopping_rounds=550,\n",
    "                           loss_function='CrossEntropy',\n",
    "#                             depth=8,\n",
    "                           )\n",
    "        cbt_model.fit(train_pool, eval_set=eval_pool, verbose=100)\n",
    "#         with open('./models/fold%d_cbt_v1.mdl' % index, 'wb') as file:\n",
    "#             pickle.dump(cbt_model, file)\n",
    "#     else:\n",
    "#         with open('./models/fold%d_cbt_v1.mdl' % index, 'rb') as file:\n",
    "#             cbt_model = pickle.load(file)\n",
    "    \n",
    "    imp['score%d' % (index+1)] = cbt_model.feature_importances_\n",
    "    \n",
    "    val_dat['pred'] = cbt_model.predict_proba(X_valid)[:, 1]\n",
    "    val_pred_dict = gen_dict(val_dat, 'pred')\n",
    "    val_true_dict = gen_dict(val_dat, 'label')\n",
    "    precision, recall, f1 = f1_score(val_pred_dict, val_true_dict)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    score = cbt_model.best_score_['validation']['AUC']\n",
    "    scores.append(score)\n",
    "    print('fold %d round %d : auc: %.6f | mean auc %.6f | F1: %.6f | mean F1: %.6f' % (index+1, cbt_model.best_iteration_, score,np.mean(scores), f1, np.mean(f1_scores))) \n",
    "    preds += cbt_model.predict_proba(test_x)  \n",
    "#     break\n",
    "    del cbt_model, train_pool, eval_pool\n",
    "    del X_train, y_train, X_valid, y_valid\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "#     mdls.append(cbt_model)\n",
    "\n",
    "imp.sort_values(by='score1', ascending=False)\n",
    "\n",
    "test_data['pred'] = preds[:, 1]\n",
    "\n",
    "test_data.head()\n",
    "\n",
    "result_dict = gen_dict(test_data, 'pred')\n",
    "\n",
    "len(result_dict)\n",
    "\n",
    "import json\n",
    "import time\n",
    "localtime = time.localtime(time.time())\n",
    "save_path = './out/result_%02d%02d%02d%02d%02d.json' % (localtime[1], localtime[2], localtime[3], localtime[4],localtime[5])\n",
    "with open(save_path, 'w') as file:\n",
    "    file.write(json.dumps(result_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                       Type             Data/Info\n",
      "---------------------------------------------------------\n",
      "CatBoostClassifier             type             <class 'catboost.core.CatBoostClassifier'>\n",
      "KFold                          ABCMeta          <class 'sklearn.model_selection._split.KFold'>\n",
      "LabelEncoder                   type             <class 'sklearn.preproces<...>sing.label.LabelEncoder'>\n",
      "OrderedDict                    type             <class 'collections.OrderedDict'>\n",
      "Pool                           type             <class 'catboost.core.Pool'>\n",
      "aid                            str_             Ms4yXx88\n",
      "an                             str              Jian-Qin Sun\n",
      "ao                             OrderedDict      OrderedDict([('name', 'Ji<...>partment of Nutrition')])\n",
      "author_id                      str              3qn5CxG9\n",
      "author_ids                     list             n=11\n",
      "author_ids_sample              set              {'Izy4g5GJ', 'vwt9Wd81', <...>, 'xOEKHNxv', 'Ms4yXx88'}\n",
      "author_name                    str              hiroshi_sakamoto\n",
      "author_name_map                dict             n=2164\n",
      "author_name_paper_ids          DataFrame              author_id   author_<...>n[48750 rows x 3 columns]\n",
      "author_names                   list             n=4\n",
      "author_org_map                 dict             n=320\n",
      "author_orgs                    list             n=2\n",
      "author_pub_detail              DataFrame              author_id          <...>[25911 rows x 10 columns]\n",
      "c                              str              year_b_mm2\n",
      "cate_features                  list             n=0\n",
      "check_chs                      function         <function check_chs at 0x2abb4a712b00>\n",
      "cna_valid_unass                DataFrame              paper_id author_idx<...>\\n[9650 rows x 2 columns]\n",
      "cols                           list             n=11\n",
      "convert                        function         <function convert at 0x2abb56ba67a0>\n",
      "cor                            str              Qing Yang\n",
      "data                           DataFrame                author_id author_<...>691396 rows x 19 columns]\n",
      "defaultdict                    type             <class 'collections.defaultdict'>\n",
      "dis                            list             n=2\n",
      "drop_feat                      list             n=5\n",
      "f1                             float64          0.945187122567609\n",
      "f1_score                       function         <function f1_score at 0x2abc0af72ef0>\n",
      "f1_scores                      list             n=10\n",
      "feat_dir                       str              ./feat/\n",
      "feats                          list             n=2\n",
      "file                           TextIOWrapper    <_io.TextIOWrapper name='<...>ode='w' encoding='UTF-8'>\n",
      "func                           function         <function func at 0x2abbfb8c5e60>\n",
      "gc                             module           <module 'gc' (built-in)>\n",
      "gen_dict                       function         <function gen_dict at 0x2abcba5280e0>\n",
      "has_saved                      bool             False\n",
      "i                              int              2691395\n",
      "imp                            DataFrame                                 <...>084074  \\n13   8.328055  \n",
      "index                          int              9\n",
      "json                           module           <module 'json' from '/hom<...>hon3.7/json/__init__.py'>\n",
      "k                              str              Jianmin Zhao\n",
      "kfold                          KFold            KFold(n_splits=10, random_state=42, shuffle=True)\n",
      "localtime                      struct_time      time.struct_time(tm_year=<...> tm_yday=335, tm_isdst=0)\n",
      "math                           module           <module 'math' from '/hom<...>37m-x86_64-linux-gnu.so'>\n",
      "myfunc                         function         <function myfunc at 0x2abbfb8c5ef0>\n",
      "n                              int              10\n",
      "np                             module           <module 'numpy' from '/ho<...>kages/numpy/__init__.py'>\n",
      "org                            str              Department of Nutrition\n",
      "os                             module           <module 'os' from '/home/<...>da3/lib/python3.7/os.py'>\n",
      "out_dir                        str              ./out/\n",
      "paper_authors                  dict             n=262388\n",
      "paper_authors_df               DataFrame                paper_id   author<...>[262388 rows x 2 columns]\n",
      "paper_ids                      list             n=3\n",
      "pd                             module           <module 'pandas' from '/h<...>ages/pandas/__init__.py'>\n",
      "pickle                         module           <module 'pickle' from '/h<...>lib/python3.7/pickle.py'>\n",
      "pid                            str              4F5SbAoT\n",
      "pidx                           function         <function pidx at 0x2abbf5037ef0>\n",
      "precision                      float64          0.9457777788935048\n",
      "preds                          ndarray          505765x2: 1011530 elems, type `float64`, 8092240 bytes (7.7173614501953125 Mb)\n",
      "pub_info                       DataFrame                                 <...>[271719 rows x 8 columns]\n",
      "pypinyin                       module           <module 'pypinyin' from '<...>es/pypinyin/__init__.py'>\n",
      "random                         module           <module 'random' from '/h<...>lib/python3.7/random.py'>\n",
      "recall                         float64          0.9445972035336228\n",
      "result_dict                    defaultdict      defaultdict(<class 'list'<...>CQ4wqjrv': ['zzdCBp4W']})\n",
      "save_path                      str              ./out/result_1201222936.json\n",
      "score                          float            0.9828738570213318\n",
      "scores                         list             n=10\n",
      "sys                            module           <module 'sys' (built-in)>\n",
      "test                           DataFrame                author_id  author<...>505765 rows x 19 columns]\n",
      "test_data                      DataFrame                paper_id  author_<...>[505765 rows x 5 columns]\n",
      "test_x                         DataFrame                 year_a  year_b_m<...>505765 rows x 14 columns]\n",
      "time                           module           <module 'time' (built-in)>\n",
      "time_feat_a                    DataFrame                 year_a  year_b_m<...>691396 rows x 11 columns]\n",
      "tmp                            DataFrame                 author_org_in_or<...>2691396 rows x 3 columns]\n",
      "to_pinyin                      function         <function to_pinyin at 0x2abb21ad5f80>\n",
      "tqdm_notebook                  function         <function tqdm_notebook at 0x2abb56a0eb90>\n",
      "tr_idx                         ndarray          198: 198 elems, type `int64`, 1584 bytes\n",
      "train                          DataFrame                author_id       a<...>185631 rows x 19 columns]\n",
      "train_author_ids_ext2_sample   DataFrame               author_id author_i<...>[248382 rows x 2 columns]\n",
      "train_author_name              ndarray          220: 220 elems, type `object`, 1760 bytes\n",
      "train_author_name_ids          DataFrame                  author_name    <...>n\\n[220 rows x 3 columns]\n",
      "train_author_name_ids_ext      DataFrame                    author_name a<...>n[22839 rows x 2 columns]\n",
      "train_author_name_ids_ext2     DataFrame                    author_name a<...>n[22839 rows x 4 columns]\n",
      "train_author_name_paper_ids    DataFrame              author_id          <...>n[22839 rows x 3 columns]\n",
      "train_author_paper_ids         DataFrame              author_id          <...>n[22839 rows x 2 columns]\n",
      "train_author_paper_ids_ext     DataFrame               author_id  paper_i<...>[205498 rows x 4 columns]\n",
      "train_data                     DataFrame                 paper_id       a<...>2185631 rows x 5 columns]\n",
      "train_pub_info                 DataFrame                                 <...>[203184 rows x 7 columns]\n",
      "trn_aname                      ndarray          198: 198 elems, type `object`, 1584 bytes\n",
      "trn_dat                        DataFrame                author_id author_<...>006591 rows x 19 columns]\n",
      "used_feat                      list             n=14\n",
      "v                              list             n=2\n",
      "va_idx                         ndarray          22: 22 elems, type `int64`, 176 bytes\n",
      "val_aname                      ndarray          22: 22 elems, type `object`, 176 bytes\n",
      "val_dat                        DataFrame                author_id       a<...>179040 rows x 20 columns]\n",
      "val_pred_dict                  defaultdict      defaultdict(<class 'list'<...>yoKIgw96': ['ztbMJysa']})\n",
      "val_true_dict                  defaultdict      defaultdict(<class 'list'<...>yoKIgw96': ['ztbMJysa']})\n",
      "valid_data                     DataFrame                paper_id  author_<...>[505765 rows x 4 columns]\n",
      "valid_data_                    DataFrame                paper_id  author_<...>[505765 rows x 4 columns]\n",
      "valid_pub_info                 DataFrame              paper_id           <...>\\n[9623 rows x 7 columns]\n",
      "warnings                       module           <module 'warnings' from '<...>b/python3.7/warnings.py'>\n",
      "whole_author_name_paper_ids    DataFrame              author_id   author_<...>n[25911 rows x 3 columns]\n",
      "whole_pub_info                 DataFrame                paper_id         <...>[262388 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
